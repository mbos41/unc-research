{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 79.678888\n",
      "D01012008\n",
      "start: 89.977598\n",
      "D01012009\n",
      "start: 99.644733\n",
      "D01022008\n",
      "start: 103.389865\n",
      "D01022009\n",
      "start: 113.438876\n",
      "D01032008\n",
      "start: 114.693362\n",
      "D01032009\n",
      "start: 125.453545\n",
      "D01042008\n",
      "start: 136.306633\n",
      "D01052007\n",
      "start: 148.913147\n",
      "D01052008\n",
      "start: 158.509195\n",
      "D01052009\n",
      "start: 163.00557\n",
      "D01062007\n",
      "stop: <built-in function clock>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import date, timedelta\n",
    "import time\n",
    "\n",
    "os.chdir('/Users/michaelbostwick/Documents/RA')\n",
    "\n",
    "def shift_update(shifts, prev_day, next_day):\n",
    "    shifts = shifts.filter(['agent', 'shift_id', 'shift_start', 'shift_end', 'overnight'])\n",
    "    normal = shifts[shifts.overnight==0]\n",
    "    overnight_prev = shifts[shifts.overnight==1]\n",
    "    overnight_next = shifts[shifts.overnight==2]\n",
    "    agent_shifts_prev = pd.read_csv('raw_data/{}_agent_shifts.txt'.format(prev_day), encoding=\"utf-8-sig\")\n",
    "    agent_shifts_prev = agent_shifts_prev.filter(['agent', 'shift_start', 'overnight'])\n",
    "    agent_shifts_next = pd.read_csv('raw_data/{}_agent_shifts.txt'.format(next_day), encoding=\"utf-8-sig\")\n",
    "    agent_shifts_next = agent_shifts_next.filter(['agent', 'shift_end', 'overnight'])\n",
    "    \n",
    "    prev_merge = pd.merge(overnight_prev, agent_shifts_prev, on='agent', how='left')\n",
    "    prev_merge = prev_merge[prev_merge.overnight_y != 1].sort_values(['agent', 'overnight_y'], ascending=False).drop_duplicates('agent')\n",
    "    prev_merge.loc[prev_merge.shift_start_y.isnull(), 'shift_start_y'] = prev_merge.shift_start_x[prev_merge.shift_start_y.isnull()]\n",
    "    prev_merge.drop(['shift_start_x','overnight_y'], axis=1, inplace=True)\n",
    "    prev_merge.columns = ['agent', 'shift_id', 'shift_end', 'overnight', 'shift_start']\n",
    "    \n",
    "    \n",
    "    next_merge = pd.merge(overnight_next, agent_shifts_next, on='agent', how='left')\n",
    "    next_merge = next_merge[next_merge.overnight_y != 2].sort_values(['agent', 'overnight_y'], ascending=False).drop_duplicates('agent')\n",
    "    next_merge.loc[next_merge.shift_end_y.isnull(), 'shift_end_y'] = next_merge.shift_end_x[next_merge.shift_end_y.isnull()]\n",
    "    next_merge.drop(['shift_end_x', 'overnight_y'], axis=1, inplace=True)\n",
    "    next_merge.columns = ['agent', 'shift_id', 'shift_start', 'overnight', 'shift_end']\n",
    "    \n",
    "    shifts = pd.concat([normal, prev_merge, next_merge])\n",
    "    \n",
    "    return shifts, prev_merge\n",
    "\n",
    "\n",
    "def load_merge(date, prev_day, next_day):\n",
    "    # Load csv files\n",
    "    cust_subcalls = pd.read_csv('raw_data/{}_cust_subcalls.txt'.format(date), encoding=\"utf-8-sig\")\n",
    "    agent_profile = pd.read_csv('raw_data/{}_agent_profile.txt'.format(date), encoding=\"utf-8-sig\")\n",
    "    agent_records = pd.read_csv('raw_data/{}_agent_records.txt'.format(date), encoding=\"utf-8-sig\")\n",
    "    agent_events = pd.read_csv('raw_data/{}_agent_events.txt'.format(date), encoding=\"utf-8-sig\")\n",
    "    agent_shifts = pd.read_csv('raw_data/{}_agent_shifts.txt'.format(date), encoding=\"utf-8-sig\")\n",
    "\n",
    "    # Manually add suffix to a few agent_profile columns\n",
    "    agent_profile.rename(columns={'agent': 'agent_profile', 'talk_time': 'talk_time_profile', 'consult_time': 'consult_time_profile',\n",
    "                                 'wrapup_time': 'wrapup_time_profile'}, inplace=True)\n",
    "    \n",
    "    # Reduce number of columns and manually add suffix to some columns for agent_records\n",
    "    agent_records = agent_records.filter(items=['agent', 'call_id', 'record_id', 'work_time', 'wait_time', 'ring_time',\n",
    "\t\t\t\t\t\t\t\t\t\t\t'talk_time', 'hold_time', 'wrapup_time', 'consult_time', 'consult_group', \n",
    "                                            'consult_service', 'nother_calls'])\n",
    "    agent_records.rename(columns={'agent': 'agent_records', 'talk_time': 'talk_time_records', 'consult_time': 'consult_time_records',\n",
    "                                 'wrapup_time': 'wrapup_time_records'}, inplace=True)\n",
    "    \n",
    "    \n",
    "    shifts, prev_merge = shift_update(agent_shifts, prev_day, next_day)\n",
    "    \n",
    "    \n",
    "    # Get previous day events for agents with overnight shift and add to events dataframe\n",
    "    events_prev = pd.read_csv('raw_data/D31122007_agent_events.txt', encoding=\"utf-8-sig\")\n",
    "    events_prev_keep = pd.merge(prev_merge[['agent','shift_start']], events_prev, on='agent', how='inner')\n",
    "    events_prev_keep = events_prev_keep[events_prev_keep.event_start >\n",
    "                                    events_prev_keep.shift_start].drop(['shift_start'],axis=1)\n",
    "    events_full = pd.concat([events_prev_keep, agent_events])\n",
    "    \n",
    "    # Only keep relevant agent events\n",
    "    events_full = events_full[events_full.event_id.isin([3,4,5,6,7])].filter(items=['agent', 'event_id', 'duration',\n",
    "                                                                                   'event_end'])\n",
    "    \n",
    "    # Merge on shift id to events\n",
    "    events_merge = pd.merge(events_full, shifts[['agent','shift_start','shift_end', 'shift_id']], \n",
    "                          on='agent', how='left')\n",
    "    events_shifts = events_merge[(events_merge.event_end >= events_merge.shift_start) &\n",
    "                             (events_merge.event_end <= events_merge.shift_end)]\n",
    "    events_shifts = events_shifts.drop(['shift_start', 'shift_end'], axis=1)\n",
    "     \n",
    "    # Merge on shift id to subcalls\n",
    "    subcalls_merge = pd.merge(cust_subcalls, shifts[['agent','shift_start','shift_end', 'shift_id', 'overnight']], \n",
    "                          left_on='party_answered', right_on='agent', how='left')\n",
    "\n",
    "    \n",
    "    subcalls_shifts = subcalls_merge[(subcalls_merge.party_answered == 0) | \n",
    "                                (subcalls_merge.segment_start + 1000 >= subcalls_merge.shift_start) & (subcalls_merge.segment_start <= subcalls_merge.shift_end)]\n",
    "                                # To deal with shift start lag, add 1000s of buffer\n",
    "    subcalls = subcalls_shifts.filter(items=['call_id', 'segment_start', 'party_answered', 'shift_id'])\n",
    "\n",
    "    # Merge all possible agent/event pairs\n",
    "    merge_temp = pd.merge(subcalls, events_shifts, left_on=['party_answered', 'shift_id'], \n",
    "                          right_on=['agent', 'shift_id'], how='left')\n",
    "    \n",
    "    # Only keep events occuring prior to call and aggregate\n",
    "    merge_temp_fltr = merge_temp[(merge_temp.segment_start > merge_temp.event_end)]\n",
    "    # Add fake row to make sure columns for all 5 event_id's\n",
    "    structure = pd.DataFrame({'party_answered': [99999, 99999, 99999, 99999, 99999],\n",
    "                          'event_id': [3,4,5,6,7],\n",
    "                          'duration': [0,0,0,0,0],\n",
    "                          'call_id': [1,1,1,1,1],\n",
    "                          'segment_start': [1,1,1,1,1]})\n",
    "    merge_temp_fltr =pd.concat([merge_temp_fltr, structure])\n",
    "    events = merge_temp_fltr.groupby(['call_id', 'segment_start', 'party_answered', 'event_id'])['duration'].aggregate(['count', sum]).unstack().fillna(0)\n",
    "    \n",
    "    # Rename columns\n",
    "    events.columns = events.columns.droplevel()\n",
    "    events.columns = ['event3_count', 'event4_count', 'event5_count', 'event6_count', 'event7_count',\n",
    "                      'event3_duration', 'event4_duration', 'event5_duration', 'event6_duration', 'event7_duration']\n",
    "    \n",
    "    # Merge datasets together, one at a time\n",
    "    merge1 = pd.merge(subcalls_shifts, agent_profile, left_on='party_answered', right_on='agent_profile', how='left', suffixes=[\"_cust\", \"_profile\"])\n",
    "        \n",
    "    merge2 = pd.merge(merge1, agent_records, left_on=['call_id', 'record_id', 'party_answered'], right_on=['call_id', 'record_id', 'agent_records'], how='left', suffixes=[\"_cust\", \"_records\"])\n",
    "\n",
    "    merged = pd.merge(merge2, events, left_on=['call_id', 'segment_start', 'party_answered'], right_index=True, how='left')\n",
    "    \n",
    "    # Add time columns\n",
    "    merged['time_since_in'] = merged.segment_start - merged.shift_start\n",
    "    merged['time_until_out'] = merged.shift_end - merged.segment_start\n",
    "    merged['time'] = pd.to_datetime(merged.segment_start,unit='s')\n",
    "    merged['contact_year'], merged['contact_month'] = merged.time.dt.year, merged.time.dt.month\n",
    "    merged['contact_day'], merged['contact_hour'] = merged.time.dt.day, merged.time.dt.hour\n",
    "    merged.drop('time', axis=1, inplace=True)\n",
    "                  \n",
    "    return merged\n",
    "\n",
    "# Create list of all unique days in folder\n",
    "def file_list():\n",
    "    files = os.listdir('raw_data/')\n",
    "\n",
    "    files_short = []\n",
    "    prev = ''\n",
    "    for file in files:\n",
    "        current = file[:9]\n",
    "        if current != prev and current[0] != '.':\n",
    "            files_short.append(current)\n",
    "        prev = current\n",
    "    \n",
    "    return files_short\n",
    " \n",
    "days = file_list()\n",
    "days = days[11:]\n",
    "num_files = len(days)\n",
    "\n",
    "#days = ['D01032008']\n",
    "months = ['January', 'February', 'March', 'April', 'May', 'June', 'July',\n",
    "          'August', 'September', 'October', 'November', 'December']\n",
    " \n",
    "start = time.clock()\n",
    "# Loop through each day and export merged csv file to appropriate month folder   \n",
    "for i, day in enumerate(days):\n",
    "    month = months[int(day[3:5])-1]\n",
    "    year = day[-4:]\n",
    "    prev_date = date(int(year), int(day[3:5]), int(day[1:3])) - timedelta(days=1)\n",
    "    next_date = date(int(year), int(day[3:5]), int(day[1:3])) + timedelta(days=1)\n",
    "    yesterday = 'D' + str(prev_date.day).rjust(2,'0') + str(prev_date.month).rjust(2,'0') + str(prev_date.year)\n",
    "    tomorrow = 'D' + str(next_date.day).rjust(2,'0') + str(next_date.month).rjust(2,'0') + str(next_date.year)\n",
    "    print(day, ' ', i, '/', num_files) \n",
    "    merged = load_merge(day, yesterday, tomorrow) \n",
    "    merged.to_csv('clean_data/{}{}/{}_merged.csv'.format(month,year,day), index = False)\n",
    "\n",
    "stop = time.clock()\n",
    "print('time elapsed:', (stop - start) // 60, 'mins')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
